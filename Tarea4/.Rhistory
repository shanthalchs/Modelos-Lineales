library(knitr)
library(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
cache=TRUE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
datos <- read.table("datos/AsientosNinos.csv")
datos <- read.table("datos/AsientosNinnos.csv")
datos <- read.table("datos\AsientosNinnos.csv",)
datos <- read.table("datos/AsientosNinnos.csv",)
datos <- read.table("datos/AsientosNinnos.csv", dec = ".", sep = ",",header = T, stringsAsFactors = T)
datos <- read.table("datos/AsientosNinnos.csv", dec = ".", sep = ",",header = T, stringsAsFactors = T)
setwd("~/Github/Modelos-Lineales/Tarea4")
datos <- read.table("datos/AsientosNinnos.csv", dec = ".", sep = ",",header = T, stringsAsFactors = T)
datos <- read.table("datos/AsientosNinno.csv", dec = ".", sep = ",",header = T, stringsAsFactors = T)
View(datos)
datos <- read.table("datos/AsientosNinno.csv", dec = ".", sep = ";",header = T, stringsAsFactors = T)
datos <- read.table("datos/AsientosNinno.csv", dec = ".", sep = ";",header = T, stringsAsFactors = T)[,-1]
str(datos)
datos <- read.table("datos/AsientosNinno.csv", dec = ".", sep = ";",header = T, stringsAsFactors = T)[,-1]
str(datos)
numero.predictoras <- dim(datos)[2] - 1
filas <- dim(datos)[1]
muestra <- sample(1:filas, floor(filas*0.20))
ttesting <- datos[muestra,]
taprendizaje <- datos[-muestra,]
datos$CalidadEstant <- factor(datos$CalidadEstant,levels = c("Malo","Medio","Bueno"), ordered=TRUE)
str(datos)
datos <- read.table("datos/AsientosNinno.csv", dec = ".", sep = ";",header = T, stringsAsFactors = T)[,-1]
datos$CalidadEstant <- factor(datos$CalidadEstant,levels = c("Malo","Medio","Bueno"), ordered=TRUE)
str(datos)
numero.predictoras <- dim(datos)[2] - 1
filas <- dim(datos)[1]
muestra <- sample(1:filas, floor(filas*0.20))
ttesting <- datos[muestra,]
taprendizaje <- datos[-muestra,]
library(rpart)
library(rpart.plot)
library(ggplot2)
modelo.rpart <- rpart(Ventas~., data = taprendizaje, control = rpart.control(minsplit = 5, maxdepth = 25))
prediccion1 <- predict(modelo.rpart, ttesting)
Arboles <- indices.precision(ttesting$Ventas,prediccion1,numero.predictoras)
# Residual Sum of Square (RSS)
RSS <- function(Pred,Real) {
ss <- sum((Real-Pred)^2)
return(ss)
}
RSE<-function(Pred,Real,NumPred) {
N<-length(Real)-NumPred-1  # <- length(Real)-(NumPred+1)
ss<-sqrt((1/N)*RSS(Pred,Real))
return(ss)
}
MSE <- function(Pred,Real) {
N<-length(Real)
ss<-(1/N)*RSS(Pred,Real)
return(ss)
}
error.relativo <- function(Pred,Real) {
ss<-sum(abs(Real-Pred))/sum(abs(Real))
return(ss)
}
# Funciones para desplegar precisión
indices.precision <- function(real, prediccion,cantidad.variables.predictoras) {
return(list(error.cuadratico = MSE(prediccion,real),
raiz.error.cuadratico = RSE(prediccion,real,cantidad.variables.predictoras),
error.relativo = error.relativo(prediccion,real),
correlacion = as.numeric(cor(prediccion,real))))
}
# Gráfico de dispersión entre el valor real de la variable a predecir y la predicción del modelo.
plot.real.prediccion <- function(real, prediccion, modelo = "") {
g <- ggplot(data = data.frame(Real = real, Prediccion = as.numeric(prediccion)), mapping = aes(x = Real, y = Prediccion)) +
geom_point(size = 1, col = "dodgerblue3") +
labs(title = paste0("Real vs Predicción", ifelse(modelo == "", "", paste(", con", modelo))),
x = "Real",
y = "Predicción")
return(g)
}
Arboles <- indices.precision(ttesting$Ventas,prediccion1,numero.predictoras)
Arboles
#elimino las variables
taprendizaje <- taprendizaje[,-c(4,13)]
ttesting <- ttesting[,-c(4,13)]
library(rpart)
library(rpart.plot)
library(ggplot2)
modelo.rpart <- rpart(Ventas~., data = taprendizaje, control = rpart.control(minsplit = 5, maxdepth = 25))
prediccion1 <- predict(modelo.rpart, ttesting)
Arboles <- indices.precision(ttesting$Ventas,prediccion1,numero.predictoras)
Arboles
print(asRules(modelo.rpart))
library(rattle)
print(asRules(modelo.rpart))
library(randomForest)
modelo.randomForest <- randomForest(Ventas~., data =taprendizaje, importance = TRUE, ntree = 200, mtry = 3)
prediccion2 <- predict(modelo.randomForest, ttesting)
Bosques <- indices.precision(ttesting$Ventas,prediccion2,numero.predictoras)
Bosques
ggVarImp(modelo.rf)
ggVarImp(modelo.randomForest)
library(gbm)
modelo.potenciacion <- gbm(Ventas~., data = taprendizaje, distribution = "gaussian",
interaction.depth = 1, n.trees = 200, shrinkage = 0.1)
prediccion3 <- predict(modelo.potenciacion , ttesting)
Potenciacion.G <- indices.precision(ttesting$Ventas,prediccion3,numero.predictoras)
Potenciacion.G
library(gbm)
modelo.potenciacion2 <- gbm(Balance~., data = taprendizaje, distribution = "laplace",
interaction.depth = 1, n.trees = 300, shrinkage = 0.1)
library(gbm)
modelo.potenciacion2 <- gbm(Ventas~., data = taprendizaje, distribution = "laplace",
interaction.depth = 1, n.trees = 300, shrinkage = 0.1)
prediccion4 <- predict(modelo.potenciacion2 , ttesting)
Potenciacion.L <- indices.precision(ttesting$Ventas,prediccion4,numero.predictoras)
Potenciacion.L
library(gbm)
modelo.potenciacion2 <- gbm(Ventas~., data = taprendizaje, distribution = "laplace",
interaction.depth = 1, n.trees = 200, shrinkage = 0.1)
prediccion4 <- predict(modelo.potenciacion2 , ttesting)
Potenciacion.L <- indices.precision(ttesting$Ventas,prediccion4,numero.predictoras)
Potenciacion.L
library(gbm)
modelo.potenciacion3 <- gbm(Ventas~., data = taprendizaje, distribution = "bernoulli",
interaction.depth = 1, n.trees = 200, shrinkage = 0.1)
prediccion5 <- predict(modelo.potenciacion3, ttesting)
Potenciacion.B <- indices.precision(ttesting$Ventas,prediccion5,numero.predictoras)
Potenciacion.B
library(gbm)
modelo.potenciacion3 <- gbm(Ventas~., data = taprendizaje, distribution = "bernoulli",
interaction.depth = 1, n.trees = 200, shrinkage = 0.1)
prediccion5 <- predict(modelo.potenciacion3, ttesting)
Potenciacion.B <- indices.precision(ttesting$Ventas,prediccion5,numero.predictoras)
Potenciacion.B
library(gbm)
modelo.potenciacion3 <- gbm(Ventas~., data = taprendizaje, distribution = "bernoulli",
interaction.depth = 1, n.trees = 200, shrinkage = 0.1)
prediccion5 <- predict(modelo.potenciacion3, ttesting)
Potenciacion.B <- indices.precision(ttesting$Ventas,prediccion5,numero.predictoras)
Potenciacion.B
library(gbm)
modelo.potenciacion3 <- gbm(Ventas~., data = taprendizaje, distribution = "poisson",
interaction.depth = 1, n.trees = 200, shrinkage = 0.1)
library(ggplot2)
ggplot(summary(modelo.potenciacion,plot=FALSE), aes(x = fct_reorder(var, rel.inf), y = rel.inf, fill = fct_reorder(var, rel.inf))) +
geom_bar(stat = "identity", position = "identity", width = 0.1) +
labs(title = "Importancia de Variables según Influencia Relativa", y = "Influencia Relativa", x = "") +
scale_y_continuous(labels = scales::comma) +
coord_flip() +
theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
library(ggplot2)
library(tidyverse)
ggplot(summary(modelo.potenciacion,plot=FALSE), aes(x = fct_reorder(var, rel.inf), y = rel.inf, fill = fct_reorder(var, rel.inf))) +
geom_bar(stat = "identity", position = "identity", width = 0.1) +
labs(title = "Importancia de Variables según Influencia Relativa", y = "Influencia Relativa", x = "") +
scale_y_continuous(labels = scales::comma) +
coord_flip() +
theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
library(knitr)
library(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
cache=TRUE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
# Residual Sum of Square (RSS)
RSS <- function(Pred,Real) {
ss <- sum((Real-Pred)^2)
return(ss)
}
RSE<-function(Pred,Real,NumPred) {
N<-length(Real)-NumPred-1  # <- length(Real)-(NumPred+1)
ss<-sqrt((1/N)*RSS(Pred,Real))
return(ss)
}
MSE <- function(Pred,Real) {
N<-length(Real)
ss<-(1/N)*RSS(Pred,Real)
return(ss)
}
error.relativo <- function(Pred,Real) {
ss<-sum(abs(Real-Pred))/sum(abs(Real))
return(ss)
}
# Funciones para desplegar precisión
indices.precision <- function(real, prediccion,cantidad.variables.predictoras) {
return(list(error.cuadratico = MSE(prediccion,real),
raiz.error.cuadratico = RSE(prediccion,real,cantidad.variables.predictoras),
error.relativo = error.relativo(prediccion,real),
correlacion = as.numeric(cor(prediccion,real))))
}
# Gráfico de dispersión entre el valor real de la variable a predecir y la predicción del modelo.
plot.real.prediccion <- function(real, prediccion, modelo = "") {
g <- ggplot(data = data.frame(Real = real, Prediccion = as.numeric(prediccion)), mapping = aes(x = Real, y = Prediccion)) +
geom_point(size = 1, col = "dodgerblue3") +
labs(title = paste0("Real vs Predicción", ifelse(modelo == "", "", paste(", con", modelo))),
x = "Real",
y = "Predicción")
return(g)
}
datos <- read.table("datos/wine.csv", dec = ".", sep = ",",header = T, stringsAsFactors = T)
filas <- dim(datos)[1]
muestra <- sample(1:filas, floor(filas*0.20))
ttesting <- datos[muestra,]
taprendizaje <- datos[-muestra,]
library(rpart)
library(traineR)
library(rpart.plot)
modelo <- train.rpart(tipo~.,data = taprendizaje, minsplit = 2)
prediccion <- predict(modelo, ttesting, type = 'class')
mc         <- confusion.matrix(newdata = ttesting, prediccion)
Arboles <- general.indexes(mc = mc)
Arboles
prp(modelo,extra=104,
branch.type=2,
box.col=c("pink",
"palegreen3",
"cyan")[modelo$frame$yval])
modelo.Bosques  <- train.randomForest(formula = tipo~., data = taprendizaje, importance = T)
prediccion.Bosques   <- predict(modelo.Bosques, ttesting, type = "class")
mc           <- confusion.matrix(ttesting, prediccion.Bosques)
BosquesAleatorios <- general.indexes(mc = mc)
BosquesAleatorios
modelo.ADA  <- train.ada(formula = tipo~.,data = taprendizaje)
prediccion.ADA   <- predict(modelo.ADA, ttesting, type = "class")
mc           <- confusion.matrix(ttesting, prediccion.ADA)
ADABoosting <- general.indexes(mc = mc)
ADABoosting
modelo.XG <- train.xgboost(formula = tipo~.,
data    = taprendizaje,
nrounds = 79,
verbose = F)
prediccion.XG <- predict(modelo.XG, ttesting , type = "class")
mc         <- confusion.matrix(ttesting,prediccion.XG)
XGBoosting <- general.indexes(mc = mc)
XGBoosting
Confusion <- function(MC){
VN <- MC[1,1]
FN <- MC[2,1]
VP <- MC[2,2]
FP <- MC[1,2]
PG <- (VN + VP)/(VN + FP + FN + VP)
EG <- 1 - PG
PP <- VP/(FN + VP)
PN <- VN/(FP + VN)
FP <- FP/(VN + FP)
FN <- FN/(VP + FN)
AP <- VP/(FP + VP)
AN <- VN/(VN + FN)
Respuesta <- list(PG = PG,EG = EG, PP = PP,PN = PN,FP = FP,FN=FN,AP=AP,AN=AN)
names(Respuesta) <- c("Precision Global", "Error Global","Precision Positiva", "Precision Negativa", "Falsos Positivos", "Falsos Negativos", "Asertividad Positiva" , "Asertividad Negativa")
return(Respuesta)
}
precisiones <- rbind(as.data.frame(Confusion(Arboles$confusion.matrix)),as.data.frame(Confusion(BosquesAleatorios$confusion.matrix)),as.data.frame(Confusion(ADABoosting$confusion.matrix)),as.data.frame(Confusion(XGBoosting$confusion.matrix)))
tablaC <- read.table("Tabla Comparativa wine.csv",dec = ".",sep = "," , header = T)
tablaC <- rbind(precisiones,tablaC)
rownames(tablaC) <- c("Árboles de Decisión", "Bosques Aleatorios", "ADA Boosting", "XG Boosting","rectangular", "triangular", "epanechnikov", "biweight", "triweight", "cos",
"inv", "gaussian","optimal")
tablaC
View(tablaC)
tablaC <- read.table("Tabla Comparativa wine.csv",dec = ".",sep = "," , header = T)
tablaC
tablaC <- read.table("Tabla Comparativa wine.csv",dec = ".",sep = "," , header = T)
tablaC
tablaC <- read.table("Tabla Comparativa wine.csv",dec = ".",sep = "," , header = T)
tablaC
precisiones <- rbind(as.data.frame(Confusion(Arboles$confusion.matrix)),as.data.frame(Confusion(BosquesAleatorios$confusion.matrix)),as.data.frame(Confusion(ADABoosting$confusion.matrix)),as.data.frame(Confusion(XGBoosting$confusion.matrix)))
tablaC <- read.table("Tabla Comparativa wine.csv",dec = ".",sep = "," , header = T)
tablaC <- rbind(precisiones,tablaC)
rownames(tablaC) <- c("Árboles de Decisión", "Bosques Aleatorios", "ADA Boosting", "XG Boosting","rectangular", "triangular", "epanechnikov", "biweight", "triweight", "cos",
"inv", "gaussian","optimal")
tablaC %>%
arrange(desc(Precision.Global))
write.csv(tablaC,"Tabla Comparativa wine.csv", row.names = FALSE)
tablaC
